#  Projet Offres dâ€™Emplois â€“ Data Engineering

## Structure du projet

```bash
offres_emplois/
â”‚â”€â”€ Airflow/                  # Orchestration des workflows ETL avec Apache Airflow
â”‚â”€â”€ Datalakes/                # Stockage brut des donnÃ©es (Hadoop / S3)
â”‚â”€â”€ Dossiers_json_excel_csv/  # Export des donnÃ©es sous formats JSON, Excel, CSV
â”‚â”€â”€ Scraping_job/             # Scripts de scraping (Scrapy)
â”‚â”€â”€ basesDeDonnees/           # Scripts de configuration / initialisation des bases (MySQL, Postgresâ€¦)
â”‚â”€â”€ docker-compose.yaml       # Configuration des services Docker (Airflow, DBs)
â”‚â”€â”€ requirements.txt          # DÃ©pendances Python du projet
â”‚â”€â”€ src/                      # Code source Python 
```

---

## ğŸš€ Objectif du projet

Mettre en place une **pipeline complÃ¨te de collecte et dâ€™analyse des offres dâ€™emplois au sengal** :

Voici une version rÃ©Ã©crite, plus claire et professionnelle de ton texte ğŸ‘‡

---

1. **Scraping** des donnÃ©es depuis diffÃ©rentes plateformes, notamment **EmploiSenegal.com** et **Senjob**.

2. **Stockage des donnÃ©es dans diffÃ©rentes bases** : **MySQL, PostgreSQL et MongoDB**.

   * AprÃ¨s le scraping, les donnÃ©es sont dâ€™abord enregistrÃ©es dans plusieurs dossiers sous diffÃ©rents formats (**CSV, JSON, Excel**) afin de simuler un environnement oÃ¹ les fichiers sont dispersÃ©s.
   * Le pipeline permet ensuite dâ€™**ingÃ©rer automatiquement** ces fichiers dans les bases de donnÃ©es correspondantes.
   * Chaque fois quâ€™un nouveau fichier (CSV, Excel ou JSON) est gÃ©nÃ©rÃ©, il est automatiquement associÃ© et insÃ©rÃ© dans une base de donnÃ©es.

3. **Stockage brut dans un Data Lake** : toutes les donnÃ©es prÃ©sentes dans les diffÃ©rentes bases sont ensuite **centralisÃ©es dans un Data Lake Hadoop**, sans traitement prÃ©alable, afin de conserver une copie brute et uniforme des informations.

---
4. **Transformation et nettoyage** des donnÃ©es (ETL).

5. **Orchestration** et automatisation avec **Airflow**.
6. **Visualisation & analyse**.

---
##Â creer un espace virtuelle 


```bash
python3 -m venv .benv     # <--- 
source .benv/bin/activate 
```

## ğŸ“¦ Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

---
## ğŸ³ DÃ©marrage avec Docker

Lancer tous les services dÃ©finis dans le projet :

```bash
docker-compose up -d
```

ArrÃªter les services :

```bash
docker-compose down
```

---


## ğŸ”§ Technologies utilisÃ©es

* **Scrapy** â†’ Collecte de donnÃ©es
* **PostgreSQL / MySQL /MongoDB** â†’ Stockage bases de donnÃ©es 
* **Hadoop / S3** â†’ Data Lake
* **PySpark** Ingection des bases de donnÃ©es vers hadoop
* **Apache Airflow** â†’ Orchestration des workflows
* **Docker & Docker Compose** â†’ Conteneurisation

---

## ğŸ“Š Cas dâ€™usage

* Suivi des tendances du marchÃ© de lâ€™emploi
* Analyse des compÃ©tences les plus demandÃ©es
* Visualisation des salaires et gÃ©olocalisation des offres

---
